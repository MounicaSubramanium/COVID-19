Potential modification to make:

* Write csv files with each pull, instead of only at the end of execution.
* Adjust the main loop such that it only
  does data pull if the cell == 0.
* With the two above points, one should be able
  to resume a run of data pulls in the case of some
  sort of runtime error that interrupts execution.
Done

* writing to csv with each API call is too slow, it turns out.
  It should probably be modified to write every 1,000 calls or so.
Done

* Maybe remove entries with coordinates of 0,0. There are a bunch of them, and
  they are meaningless for these purposes here, and they wind up costing
  hundreds, if not thousands, of API calls (and time)

* Even with writing files after ever 1,000 calls, this isn't fast enough.
  Some of the CSV files are approaching 10MB in size, if not bigger.
  Maybe I can do the file writing with a parallel thread or process or
  something.
