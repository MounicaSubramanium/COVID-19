{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whereas the other notebook is to create a template from the JHU data and start making\n",
    "# API calls from scratch, this one will pick up where previous calls left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# reading CSV files to create dataframes\n",
    "# df_confirmed = pd.read_csv('./csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv')\n",
    "\n",
    "df_tMax = pd.read_csv('./csv/tMax_US.csv')\n",
    "df_tMin = pd.read_csv('./csv/tMin_US.csv')\n",
    "df_humidity = pd.read_csv('./csv/humidity_US.csv')\n",
    "df_uvIndex = pd.read_csv('./csv/uv_US.csv')\n",
    "df_cloud = pd.read_csv('./csv/cloud_US.csv')\n",
    "df_precipprob = pd.read_csv('./csv/precip_US.csv')\n",
    "df_dewpoint = pd.read_csv('./csv/dew_US.csv')\n",
    "df_pressure = pd.read_csv('./csv/pressure_US.csv')\n",
    "df_windspeed = pd.read_csv('./csv/wind_US.csv')\n",
    "df_ozone = pd.read_csv('./csv/ozone_US.csv')\n",
    "df_sunrise = pd.read_csv('./csv/sunrise_US.csv')\n",
    "df_sunset = pd.read_csv('./csv/sunset_US.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tMax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resuming the API calls\n",
    "\n",
    "# Code to increase number of retries on connection errors,\n",
    "# and also to give it some time.\n",
    "# Found on https://stackoverflow.com/questions/15431044/can-i-set-max-retries-for-requests-request\n",
    "# And https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/\n",
    "\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "retries = Retry(total=30,\n",
    "                backoff_factor=0.1,\n",
    "                status_forcelist=[ 429, 500, 502, 503, 504 ],\n",
    "                method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"])\n",
    "\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", adapter)\n",
    "http.mount(\"http://\", adapter)\n",
    "\n",
    "# pull data from darksky weather API\n",
    "\n",
    "# Columns to be skipped when iterating through the DataFrame\n",
    "do_not_include = ['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Province_State',\n",
    "                 'Country_Region', 'Lat', 'Long_', 'Combined_Key']\n",
    "\n",
    "#Darksky API key\n",
    "API_KEY = '723a6f9dbda64ae1e0b9fdde14ba752e'\n",
    "\n",
    "# counter\n",
    "counter = 0\n",
    "\n",
    "# Dummy value in case of errors\n",
    "dummy = -1000\n",
    "\n",
    "# variable for determining how many API calls between writing data to CSV\n",
    "write_var = 1000\n",
    "\n",
    "# Start iterating through the date columns\n",
    "for x in df_tMax.columns.values:\n",
    "    \n",
    "    # Skip the columns that are not dates\n",
    "    if (x not in do_not_include):\n",
    "        \n",
    "        # Create Unix time stamp out of the date column\n",
    "        t = pd.to_datetime(df_tMax[x].name)\n",
    "        t = int(t.value / 10**9)\n",
    "        t = str(t)\n",
    "                \n",
    "        # Start iterating through the rows (locations)\n",
    "        for y in range(df_tMax['1/22/20'].values.size):\n",
    "            \n",
    "            # Only do API call if the cell value is 0\n",
    "            if str(df_tMax.iloc[y][x]) == '-10000':\n",
    "                \n",
    "                print('Cell is -10000')\n",
    "                \n",
    "                # latitude and longitude coordinates of the row to be passed to the API            \n",
    "                latitude = str(df_tMax.iloc[y][8])\n",
    "                longitude = str(df_tMax.iloc[y][9])\n",
    "\n",
    "                # Building the URL for the API get\n",
    "                url = 'https://api.darksky.net/forecast/' + API_KEY + '/' + latitude + \",\" + longitude + ',' + t\n",
    "                url = url + '?exclude=currently,flags&units=si'\n",
    "\n",
    "                # Getting the API call\n",
    "                # using the retry error handling established above\n",
    "                response = http.get(url)\n",
    "                \n",
    "                # Putting the API response into the JSON thing\n",
    "                info = json.loads(response.content)\n",
    "\n",
    "                # adding error handling in case something is wrong with the JSON response\n",
    "                try:\n",
    "\n",
    "                    # Making a variable to more easily acccess JSON response data\n",
    "                    easy_info = info['daily']['data'][0]\n",
    "\n",
    "                    # Reading the JSON data\n",
    "                    tMax = easy_info['temperatureHigh']\n",
    "                    tMin = easy_info['temperatureLow']\n",
    "                    hum = easy_info['humidity'] * 100\n",
    "                    uvee = easy_info['uvIndex']\n",
    "                    clouds = easy_info['cloudCover'] * 100\n",
    "                    precip = easy_info['precipProbability'] * 100\n",
    "                    dew = easy_info['dewPoint']\n",
    "                    pressure = easy_info['pressure']\n",
    "                    wind = easy_info['windSpeed']\n",
    "                    ozone = easy_info['ozone']\n",
    "                    sunrise = easy_info['sunriseTime']\n",
    "                    sunset = easy_info['sunsetTime']\n",
    "\n",
    "                except:\n",
    "\n",
    "                    # Creating dummy values in case of error\n",
    "                    print('Error encountered')\n",
    "                    tMax = dummy\n",
    "                    tMin = dummy\n",
    "                    hum = dummy\n",
    "                    uvee = dummy\n",
    "                    clouds = dummy\n",
    "                    precip = dummy\n",
    "                    dew = dummy\n",
    "                    pressure = dummy\n",
    "                    wind = dummy\n",
    "                    ozone = dummy\n",
    "                    sunrise = dummy\n",
    "                    sunset = dummy\n",
    "\n",
    "                # Recording the data into the respective dataframes\n",
    "                df_tMax.at[y, x] = tMax\n",
    "                df_tMin.at[y, x] = tMin\n",
    "                df_humidity.at[y, x] = hum\n",
    "                df_uvIndex.at[y, x] = uvee\n",
    "                df_cloud.at[y, x] = clouds\n",
    "                df_precipprob.at[y, x] = precip\n",
    "                df_dewpoint.at[y, x] = dew\n",
    "                df_pressure.at[y, x] = pressure\n",
    "                df_windspeed.at[y, x] = wind\n",
    "                df_ozone.at[y, x] = ozone\n",
    "                df_sunrise.at[y,x] = sunrise\n",
    "                df_sunset.at[y,x] = sunset\n",
    "            \n",
    "            counter = counter + 1\n",
    "            print(counter)\n",
    "\n",
    "            # writing CSVs of what I've got so far, for every write_var API calls\n",
    "            if counter % write_var == 0:\n",
    "                \n",
    "                print('1000 API calls')\n",
    "                df_tMax.to_csv('./csv/tMax_US.csv', index=False)\n",
    "                df_tMin.to_csv('./csv/tMin_US.csv', index=False)\n",
    "                df_humidity.to_csv('./csv/humidity_US.csv', index=False)\n",
    "                df_uvIndex.to_csv('./csv/uv_US.csv', index=False)\n",
    "                df_cloud.to_csv('./csv/cloud_US.csv', index=False)\n",
    "                df_precipprob.to_csv('./csv/precip_US.csv', index=False)\n",
    "                df_dewpoint.to_csv('./csv/dew_US.csv', index=False)\n",
    "                df_pressure.to_csv('./csv/pressure_US.csv', index=False)\n",
    "                df_windspeed.to_csv('./csv/wind_US.csv', index=False)\n",
    "                df_ozone.to_csv('./csv/ozone_US.csv', index=False)\n",
    "                df_sunrise.to_csv('./csv/sunrise_US.csv', index=False)\n",
    "                df_sunset.to_csv('./csv/sunset_US.csv', index=False)\n",
    "\n",
    "# Writing final data to csv\n",
    "print('Final data write')\n",
    "df_tMax.to_csv('./csv/tMax_US.csv', index=False)\n",
    "df_tMin.to_csv('./csv/tMin_US.csv', index=False)\n",
    "df_humidity.to_csv('./csv/humidity_US.csv', index=False)\n",
    "df_uvIndex.to_csv('./csv/uv_US.csv', index=False)\n",
    "df_cloud.to_csv('./csv/cloud_US.csv', index=False)\n",
    "df_precipprob.to_csv('./csv/precip_US.csv', index=False)\n",
    "df_dewpoint.to_csv('./csv/dew_US.csv', index=False)\n",
    "df_pressure.to_csv('./csv/pressure_US.csv', index=False)\n",
    "df_windspeed.to_csv('./csv/wind_US.csv', index=False)\n",
    "df_ozone.to_csv('./csv/ozone_US.csv', index=False)\n",
    "df_sunrise.to_csv('./csv/sunrise_US.csv', index=False)\n",
    "df_sunset.to_csv('./csv/sunset_US.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to csv\n",
    "df_tMax.to_csv('./csv/tMax_US.csv', index=False)\n",
    "df_tMin.to_csv('./csv/tMin_US.csv', index=False)\n",
    "df_humidity.to_csv('./csv/humidity_US.csv', index=False)\n",
    "df_uvIndex.to_csv('./csv/uv_US.csv', index=False)\n",
    "df_cloud.to_csv('./csv/cloud_US.csv', index=False)\n",
    "df_precipprob.to_csv('./csv/precip_US.csv', index=False)\n",
    "df_dewpoint.to_csv('./csv/dew_US.csv', index=False)\n",
    "df_pressure.to_csv('./csv/pressure_US.csv', index=False)\n",
    "df_windspeed.to_csv('./csv/wind_US.csv', index=False)\n",
    "df_ozone.to_csv('./csv/ozone_US.csv', index=False)\n",
    "df_sunrise.to_csv('./csv/sunrise_US.csv', index=False)\n",
    "df_sunset.to_csv('./csv/sunset_US.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cell\n",
    "\n",
    "# Resuming the API calls\n",
    "\n",
    "# Code to increase number of retries on connection errors,\n",
    "# and also to give it some time.\n",
    "# Found on https://stackoverflow.com/questions/15431044/can-i-set-max-retries-for-requests-request\n",
    "# And https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/\n",
    "\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "retries = Retry(total=30,\n",
    "                backoff_factor=0.1,\n",
    "                status_forcelist=[ 429, 500, 502, 503, 504 ],\n",
    "                method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"])\n",
    "\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", adapter)\n",
    "http.mount(\"http://\", adapter)\n",
    "\n",
    "# pull data from darksky weather API\n",
    "\n",
    "# Columns to be skipped when iterating through the DataFrame\n",
    "do_not_include = ['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Province_State',\n",
    "                 'Country_Region', 'Lat', 'Long_', 'Combined_Key']\n",
    "\n",
    "#Darksky API key\n",
    "API_KEY = '723a6f9dbda64ae1e0b9fdde14ba752e'\n",
    "\n",
    "# counter\n",
    "counter = 0\n",
    "\n",
    "# Dummy value in case of errors\n",
    "dummy = -1000\n",
    "\n",
    "# variable for determining how many API calls between writing data to CSV\n",
    "write_var = 1000\n",
    "\n",
    "# Start iterating through the date columns\n",
    "for x in df_tMax.columns.values:\n",
    "    \n",
    "    # Skip the columns that are not dates\n",
    "    if (x not in do_not_include):\n",
    "        \n",
    "        # Create Unix time stamp out of the date column\n",
    "        t = pd.to_datetime(df_tMax[x].name)\n",
    "        t = int(t.value / 10**9)\n",
    "        t = str(t)\n",
    "                \n",
    "        # Start iterating through the rows (locations)\n",
    "        for y in range(df_tMax['1/22/20'].values.size):\n",
    "            \n",
    "            # Only do API call if the cell value is 0\n",
    "            if str(df_tMax.iloc[y][x]) == '0':\n",
    "                \n",
    "                print('Cell is 0')\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(str(df_tMax.iloc[y][x]))\n",
    "            \n",
    "            counter = counter + 1\n",
    "            print(counter)\n",
    "\n",
    "            # writing CSVs of what I've got so far, for every write_var API calls\n",
    "            if counter % write_var == 0:\n",
    "                \n",
    "                print('1000 API calls')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
