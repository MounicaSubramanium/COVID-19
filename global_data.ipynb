{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the Hopkins COVID-19 data that was cloned from https://github.com/CSSEGISandData/COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API calls for the global data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# Loading initial Hopkins gloval time series data\n",
    "df_confirmed = pd.read_csv('./csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading API key\n",
    "\n",
    "api_file = open('key.txt', 'r')\n",
    "API_KEY = api_file.read()\n",
    "api_file.close()\n",
    "API_KEY = API_KEY.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I just need to work with the confirmed DataFrame. It's set up such that each row is a specific location, and the columns are different dates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating template for dataframes to record various values\n",
    "\n",
    "df_template = df_confirmed.copy()\n",
    "\n",
    "\n",
    "df_template.head(), df_template.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating template out of JHU table format\n",
    "\n",
    "do_not_include = ['Province/State', 'Country/Region', 'Lat', 'Long']\n",
    "\n",
    "for x in df_template.columns.values:\n",
    "    if (x not in do_not_include):\n",
    "        for y in range(df_template['8/3/20'].values.size):\n",
    "            df_template.at[y, x] = -10000\n",
    "            \n",
    "            \n",
    "df_template.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the rest of the beginning of January\n",
    "\n",
    "for q in range(1, 22):\n",
    "    day_str = str(q)\n",
    "    date_str = '1/' + day_str + '/20'\n",
    "    df_template[date_str] = -10000    \n",
    "\n",
    "df_template.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_template.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging date columns to be in chronological order\n",
    "\n",
    "ordered_columns = ['Province/State', 'Country/Region', 'Lat', 'Long', '1/1/20',\n",
    "                   '1/2/20', '1/3/20', '1/4/20', '1/5/20', '1/6/20', '1/7/20', \n",
    "                   '1/8/20', '1/9/20', '1/10/20', '1/11/20', '1/12/20', \n",
    "                   '1/13/20', '1/14/20', '1/15/20', '1/16/20', '1/17/20', \n",
    "                   '1/18/20', '1/19/20', '1/20/20', '1/21/20', '1/22/20', \n",
    "                   '1/23/20', '1/24/20', '1/25/20', '1/26/20', '1/27/20', \n",
    "                   '1/28/20', '1/29/20', '1/30/20', '1/31/20', '2/1/20', \n",
    "                   '2/2/20', '2/3/20', '2/4/20', '2/5/20', '2/6/20', '2/7/20', \n",
    "                   '2/8/20', '2/9/20', '2/10/20', '2/11/20', '2/12/20', \n",
    "                   '2/13/20', '2/14/20', '2/15/20', '2/16/20', '2/17/20', \n",
    "                   '2/18/20', '2/19/20', '2/20/20', '2/21/20', '2/22/20', \n",
    "                   '2/23/20', '2/24/20', '2/25/20', '2/26/20', '2/27/20', \n",
    "                   '2/28/20', '2/29/20', '3/1/20', '3/2/20', '3/3/20', '3/4/20', \n",
    "                   '3/5/20', '3/6/20', '3/7/20', '3/8/20', '3/9/20', '3/10/20', \n",
    "                   '3/11/20', '3/12/20', '3/13/20', '3/14/20', '3/15/20', '3/16/20',\n",
    "                   '3/17/20', '3/18/20', '3/19/20', '3/20/20', '3/21/20', '3/22/20', \n",
    "                   '3/23/20', '3/24/20', '3/25/20', '3/26/20', '3/27/20', '3/28/20', \n",
    "                   '3/29/20', '3/30/20', '3/31/20', '4/1/20', '4/2/20', '4/3/20', \n",
    "                   '4/4/20', '4/5/20', '4/6/20', '4/7/20', '4/8/20', '4/9/20', \n",
    "                   '4/10/20', '4/11/20', '4/12/20', '4/14/20', '4/15/20', '4/16/20',\n",
    "                   '4/17/20', '4/18/20', '4/19/20', '4/20/20', '4/21/20', '4/22/20', \n",
    "                   '4/23/20', '4/24/20', '4/25/20', '4/26/20', '4/27/20', '4/28/20', \n",
    "                   '4/29/20', '4/30/20', '5/1/20', '5/2/20', '5/3/20', '5/4/20', \n",
    "                   '5/5/20', '5/6/20', '5/7/20', '5/8/20', '5/9/20', '5/10/20', \n",
    "                   '5/11/20', '5/12/20', '5/13/20', '5/14/20', '5/15/20', '5/16/20',\n",
    "                   '5/17/20', '5/18/20', '5/19/20', '5/20/20', '5/21/20', '5/22/20', \n",
    "                   '5/23/20', '5/24/20', '5/25/20', '5/26/20', '5/27/20', '5/28/20', \n",
    "                   '5/29/20', '5/30/20', '5/31/20', '6/1/20', '6/2/20', '6/3/20', \n",
    "                   '6/4/20', '6/5/20', '6/6/20', '6/7/20', '6/8/20', '6/9/20', \n",
    "                   '6/10/20', '6/11/20', '6/12/20', '6/13/20', '6/14/20', '6/15/20', \n",
    "                   '6/16/20', '6/17/20', '6/18/20', '6/19/20', '6/20/20', '6/21/20', \n",
    "                   '6/22/20', '6/23/20', '6/24/20', '6/25/20', '6/26/20', '6/27/20', \n",
    "                   '6/28/20', '6/29/20', '6/30/20', '7/1/20', '7/2/20', '7/3/20', \n",
    "                   '7/4/20', '7/5/20', '7/6/20', '7/7/20', '7/8/20', '7/9/20', \n",
    "                   '7/10/20', '7/11/20', '7/12/20', '7/13/20', '7/14/20', '7/15/20', \n",
    "                   '7/16/20', '7/17/20', '7/18/20', '7/19/20', '7/20/20', '7/21/20', \n",
    "                   '7/22/20', '7/23/20', '7/24/20', '7/25/20', '7/26/20', '7/27/20', \n",
    "                   '7/28/20', '7/29/20', '7/30/20', '7/31/20', '8/1/20', '8/2/20', \n",
    "                   '8/3/20', '8/4/20', '8/5/20', '8/6/20', '8/7/20', '8/8/20', \n",
    "                   '8/9/20', '8/10/20', '8/11/20', '8/12/20']\n",
    "\n",
    "df_template = df_template[ordered_columns]\n",
    "\n",
    "df_template.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_template.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For description of the different units being captured from the API,\n",
    "# please see https://darksky.net/dev/docs\n",
    "# Requests are being made in SI units\n",
    "\n",
    "# dataframe to record median maximum temperature\n",
    "df_tMax = df_template.copy()\n",
    "\n",
    "# dataframe to record median minimum temperature\n",
    "df_tMin = df_template.copy()\n",
    "\n",
    "# dataframe to record median humidity\n",
    "df_humidity = df_template.copy()\n",
    "\n",
    "# dataframe to record median UV index\n",
    "df_uvIndex = df_template.copy()\n",
    "\n",
    "# dataframe to record median cloud cover\n",
    "df_cloud = df_template.copy()\n",
    "\n",
    "# dataframe to record weather summaries\n",
    "#df_summaries = df_template.copy()\n",
    "\n",
    "# dataframe to record precipitation probability\n",
    "df_precipprob = df_template.copy()\n",
    "\n",
    "# dataframe to record dewpoint\n",
    "df_dewpoint = df_template.copy()\n",
    "\n",
    "# dataframe to record pressure\n",
    "df_pressure = df_template.copy()\n",
    "\n",
    "# dataframe to record windspeed\n",
    "df_windspeed = df_template.copy()\n",
    "\n",
    "# dataframe to record \"ozone\" reading\n",
    "df_ozone = df_template.copy()\n",
    "\n",
    "# dataframe to record sunrise time\n",
    "df_sunrise = df_template.copy()\n",
    "\n",
    "# dataframe to record sunset time\n",
    "df_sunset = df_template.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tMax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating initial CSV files\n",
    "\n",
    "df_tMax.to_csv('./csv/tMax_Global.csv', index=False)\n",
    "df_tMin.to_csv('./csv/tMin_Global.csv', index=False)\n",
    "df_humidity.to_csv('./csv/humidity_Global.csv', index=False)\n",
    "df_uvIndex.to_csv('./csv/uv_Global.csv', index=False)\n",
    "df_cloud.to_csv('./csv/cloud_Global.csv', index=False)\n",
    "df_precipprob.to_csv('./csv/precip_Global.csv', index=False)\n",
    "df_dewpoint.to_csv('./csv/dew_Global.csv', index=False)\n",
    "df_pressure.to_csv('./csv/pressure_Global.csv', index=False)\n",
    "df_windspeed.to_csv('./csv/wind_Global.csv', index=False)\n",
    "df_ozone.to_csv('./csv/ozone_Global.csv', index=False)\n",
    "df_sunrise.to_csv('./csv/sunrise_Global.csv', index=False)\n",
    "df_sunset.to_csv('./csv/sunset_Global.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If resuming, start from below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# reading CSV files to create dataframes\n",
    "df_tMax = pd.read_csv('./csv/tMax_Global.csv')\n",
    "df_tMin = pd.read_csv('./csv/tMin_Global.csv')\n",
    "df_humidity = pd.read_csv('./csv/humidity_Global.csv')\n",
    "df_uvIndex = pd.read_csv('./csv/uv_Global.csv')\n",
    "df_cloud = pd.read_csv('./csv/cloud_Global.csv')\n",
    "df_precipprob = pd.read_csv('./csv/precip_Global.csv')\n",
    "df_dewpoint = pd.read_csv('./csv/dew_Global.csv')\n",
    "df_pressure = pd.read_csv('./csv/pressure_Global.csv')\n",
    "df_windspeed = pd.read_csv('./csv/wind_Global.csv')\n",
    "df_ozone = pd.read_csv('./csv/ozone_Global.csv')\n",
    "df_sunrise = pd.read_csv('./csv/sunrise_Global.csv')\n",
    "df_sunset = pd.read_csv('./csv/sunset_Global.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sunset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading API key\n",
    "\n",
    "api_file = open('key.txt', 'r')\n",
    "API_KEY = api_file.read()\n",
    "api_file.close()\n",
    "API_KEY = API_KEY.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resuming the API calls\n",
    "\n",
    "# Code to increase number of retries on connection errors,\n",
    "# and also to give it some time.\n",
    "# Found on https://stackoverflow.com/questions/15431044/can-i-set-max-retries-for-requests-request\n",
    "# And https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/\n",
    "\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "retries = Retry(total=30,\n",
    "                backoff_factor=0.1,\n",
    "                status_forcelist=[ 429, 500, 502, 503, 504 ],\n",
    "                method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"])\n",
    "\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", adapter)\n",
    "http.mount(\"http://\", adapter)\n",
    "\n",
    "# pull data from darksky weather API\n",
    "\n",
    "# Columns to be skipped when iterating through the DataFrame\n",
    "do_not_include = ['Province/State', 'Country/Region', 'Lat', 'Long']\n",
    "\n",
    "# counter\n",
    "counter = 0\n",
    "\n",
    "# Dummy value in case of errors\n",
    "dummy = -1000\n",
    "\n",
    "# variable for determining how many API calls between writing data to CSV\n",
    "write_var = 1000\n",
    "\n",
    "# Start iterating through the date columns\n",
    "for x in df_tMax.columns.values:\n",
    "    \n",
    "    # Skip the columns that are not dates\n",
    "    if (x not in do_not_include):\n",
    "        \n",
    "        # Create Unix time stamp out of the date column\n",
    "        t = pd.to_datetime(df_tMax[x].name)\n",
    "        t = int(t.value / 10**9)\n",
    "        t = str(t)\n",
    "                \n",
    "        # Start iterating through the rows (locations)\n",
    "        for y in range(df_tMax['1/22/20'].values.size):\n",
    "            \n",
    "            # Only do API call if the cell value is 0\n",
    "            if str(df_tMax.iloc[y][x]) == '-10000':\n",
    "                \n",
    "                print('Cell is -10000')\n",
    "                \n",
    "                # latitude and longitude coordinates of the row to be passed to the API            \n",
    "                latitude = str(df_tMax.iloc[y][2])\n",
    "                longitude = str(df_tMax.iloc[y][3])\n",
    "\n",
    "                print('Coordinates: ', latitude, ',', longitude)\n",
    "                \n",
    "                # Building the URL for the API get\n",
    "                url = 'https://api.darksky.net/forecast/' + API_KEY + '/' + latitude + \",\" + longitude + ',' + t\n",
    "                url = url + '?exclude=currently,flags&units=si'\n",
    "\n",
    "                # Getting the API call\n",
    "                # using the retry error handling established above\n",
    "                response = http.get(url)\n",
    "                \n",
    "                # Putting the API response into the JSON thing\n",
    "                info = json.loads(response.content)\n",
    "\n",
    "                # adding error handling in case something is wrong with the JSON response\n",
    "                try:\n",
    "\n",
    "                    # Making a variable to more easily acccess JSON response data\n",
    "                    easy_info = info['daily']['data'][0]\n",
    "\n",
    "                    # Reading the JSON data\n",
    "                    tMax = easy_info['temperatureHigh']\n",
    "                    tMin = easy_info['temperatureLow']\n",
    "                    hum = easy_info['humidity'] * 100\n",
    "                    uvee = easy_info['uvIndex']\n",
    "                    clouds = easy_info['cloudCover'] * 100\n",
    "                    precip = easy_info['precipProbability'] * 100\n",
    "                    dew = easy_info['dewPoint']\n",
    "                    pressure = easy_info['pressure']\n",
    "                    wind = easy_info['windSpeed']\n",
    "                    ozone = easy_info['ozone']\n",
    "                    sunrise = easy_info['sunriseTime']\n",
    "                    sunset = easy_info['sunsetTime']\n",
    "\n",
    "                except:\n",
    "\n",
    "                    # Creating dummy values in case of error\n",
    "                    print('Error encountered')\n",
    "                    tMax = dummy\n",
    "                    tMin = dummy\n",
    "                    hum = dummy\n",
    "                    uvee = dummy\n",
    "                    clouds = dummy\n",
    "                    precip = dummy\n",
    "                    dew = dummy\n",
    "                    pressure = dummy\n",
    "                    wind = dummy\n",
    "                    ozone = dummy\n",
    "                    sunrise = dummy\n",
    "                    sunset = dummy\n",
    "\n",
    "                # Recording the data into the respective dataframes\n",
    "                df_tMax.at[y, x] = tMax\n",
    "                df_tMin.at[y, x] = tMin\n",
    "                df_humidity.at[y, x] = hum\n",
    "                df_uvIndex.at[y, x] = uvee\n",
    "                df_cloud.at[y, x] = clouds\n",
    "                df_precipprob.at[y, x] = precip\n",
    "                df_dewpoint.at[y, x] = dew\n",
    "                df_pressure.at[y, x] = pressure\n",
    "                df_windspeed.at[y, x] = wind\n",
    "                df_ozone.at[y, x] = ozone\n",
    "                df_sunrise.at[y,x] = sunrise\n",
    "                df_sunset.at[y,x] = sunset\n",
    "            \n",
    "            counter = counter + 1\n",
    "            print(counter)\n",
    "\n",
    "            # writing CSVs of what I've got so far, for every write_var API calls\n",
    "            if counter % write_var == 0:\n",
    "                \n",
    "                print('1000 API calls')\n",
    "                df_tMax.to_csv('./csv/tMax_Global.csv', index=False)\n",
    "                df_tMin.to_csv('./csv/tMin_Global.csv', index=False)\n",
    "                df_humidity.to_csv('./csv/humidity_Global.csv', index=False)\n",
    "                df_uvIndex.to_csv('./csv/uv_Global.csv', index=False)\n",
    "                df_cloud.to_csv('./csv/cloud_Global.csv', index=False)\n",
    "                df_precipprob.to_csv('./csv/precip_Global.csv', index=False)\n",
    "                df_dewpoint.to_csv('./csv/dew_Global.csv', index=False)\n",
    "                df_pressure.to_csv('./csv/pressure_Global.csv', index=False)\n",
    "                df_windspeed.to_csv('./csv/wind_Global.csv', index=False)\n",
    "                df_ozone.to_csv('./csv/ozone_Global.csv', index=False)\n",
    "                df_sunrise.to_csv('./csv/sunrise_Global.csv', index=False)\n",
    "                df_sunset.to_csv('./csv/sunset_Global.csv', index=False)\n",
    "\n",
    "# Writing final data to csv\n",
    "print('Final data write')\n",
    "df_tMax.to_csv('./csv/tMax_Global.csv', index=False)\n",
    "df_tMin.to_csv('./csv/tMin_Global.csv', index=False)\n",
    "df_humidity.to_csv('./csv/humidity_Global.csv', index=False)\n",
    "df_uvIndex.to_csv('./csv/uv_Global.csv', index=False)\n",
    "df_cloud.to_csv('./csv/cloud_Global.csv', index=False)\n",
    "df_precipprob.to_csv('./csv/precip_Global.csv', index=False)\n",
    "df_dewpoint.to_csv('./csv/dew_Global.csv', index=False)\n",
    "df_pressure.to_csv('./csv/pressure_Global.csv', index=False)\n",
    "df_windspeed.to_csv('./csv/wind_Global.csv', index=False)\n",
    "df_ozone.to_csv('./csv/ozone_Global.csv', index=False)\n",
    "df_sunrise.to_csv('./csv/sunrise_Global.csv', index=False)\n",
    "df_sunset.to_csv('./csv/sunset_Global.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing final data to csv\n",
    "print('Final data write')\n",
    "df_tMax.to_csv('./csv/tMax_Global.csv', index=False)\n",
    "df_tMin.to_csv('./csv/tMin_Global.csv', index=False)\n",
    "df_humidity.to_csv('./csv/humidity_Global.csv', index=False)\n",
    "df_uvIndex.to_csv('./csv/uv_Global.csv', index=False)\n",
    "df_cloud.to_csv('./csv/cloud_Global.csv', index=False)\n",
    "df_precipprob.to_csv('./csv/precip_Global.csv', index=False)\n",
    "df_dewpoint.to_csv('./csv/dew_Global.csv', index=False)\n",
    "df_pressure.to_csv('./csv/pressure_Global.csv', index=False)\n",
    "df_windspeed.to_csv('./csv/wind_Global.csv', index=False)\n",
    "df_ozone.to_csv('./csv/ozone_Global.csv', index=False)\n",
    "df_sunrise.to_csv('./csv/sunrise_Global.csv', index=False)\n",
    "df_sunset.to_csv('./csv/sunset_Global.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tMax.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sunset.tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sunrise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tMax.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tMin.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_humidity.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uvIndex.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cloud.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precipprob.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dewpoint.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pressure.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_windspeed.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sunrise.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below modules are for testing things out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Darksky API key\n",
    "API_KEY = '723a6f9dbda64ae1e0b9fdde14ba752e'\n",
    "\n",
    "#number of seconds in one day\n",
    "one_day = 86400\n",
    "\n",
    "t = '1577836800'\n",
    "\n",
    "latitude = '15'\n",
    "longitude = '101'\n",
    "\n",
    "# sample get request \n",
    "#GET https://api.darksky.net/forecast/0123456789abcdef9876543210fedcba/\n",
    "#42.3601,-71.0589,255657600?exclude=currently,flags\n",
    "\n",
    "url = 'https://api.darksky.net/forecast/' + API_KEY + '/' + latitude + \",\" + longitude + ',' + t\n",
    "\n",
    "url = url + '?exclude=currently,flags&units=si'\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = json.loads(response.content)\n",
    "\n",
    "info['daily']['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cell\n",
    "# Resuming the API calls\n",
    "\n",
    "# Code to increase number of retries on connection errors,\n",
    "# and also to give it some time.\n",
    "# Found on https://stackoverflow.com/questions/15431044/can-i-set-max-retries-for-requests-request\n",
    "# And https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/\n",
    "\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "retries = Retry(total=30,\n",
    "                backoff_factor=0.1,\n",
    "                status_forcelist=[ 429, 500, 502, 503, 504 ],\n",
    "                method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"])\n",
    "\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", adapter)\n",
    "http.mount(\"http://\", adapter)\n",
    "\n",
    "# pull data from darksky weather API\n",
    "\n",
    "# Columns to be skipped when iterating through the DataFrame\n",
    "do_not_include = ['Province/State', 'Country/Region', 'Lat', 'Long']\n",
    "\n",
    "# Darksky API key\n",
    "# API_KEY = '8c5957c420f104bbb8f6901ff15f56d0'\n",
    "\n",
    "# counter\n",
    "counter = 0\n",
    "\n",
    "# Dummy value in case of errors\n",
    "dummy = -1000\n",
    "\n",
    "# variable for determining how many API calls between writing data to CSV\n",
    "write_var = 1000\n",
    "\n",
    "# Start iterating through the date columns\n",
    "for x in df_tMax.columns.values:\n",
    "    \n",
    "    # Skip the columns that are not dates\n",
    "    if (x not in do_not_include):\n",
    "        \n",
    "        # Create Unix time stamp out of the date column\n",
    "        t = pd.to_datetime(df_tMax[x].name)\n",
    "        t = int(t.value / 10**9)\n",
    "        # t = str(t)\n",
    "                \n",
    "        # Start iterating through the rows (locations)\n",
    "        for y in range(df_tMax['1/22/20'].values.size):\n",
    "            \n",
    "            # Only do API call if the cell value is 0\n",
    "            if str(df_tMax.iloc[y][x]) == '-10000':\n",
    "                \n",
    "                print('Cell is -10000')\n",
    "                \n",
    "                other_dummy = t + y\n",
    "                \n",
    "                tMax = other_dummy\n",
    "                tMin = other_dummy\n",
    "                hum = other_dummy\n",
    "                uvee = other_dummy\n",
    "                clouds = other_dummy\n",
    "                precip = other_dummy\n",
    "                dew = other_dummy\n",
    "                pressure = other_dummy\n",
    "                wind = other_dummy\n",
    "                ozone = other_dummy\n",
    "                sunrise = other_dummy\n",
    "                sunset = other_dummy\n",
    "\n",
    "                # Recording the data into the respective dataframes\n",
    "                df_tMax.at[y, x] = tMax\n",
    "                df_tMin.at[y, x] = tMin\n",
    "                df_humidity.at[y, x] = hum\n",
    "                df_uvIndex.at[y, x] = uvee\n",
    "                df_cloud.at[y, x] = clouds\n",
    "                df_precipprob.at[y, x] = precip\n",
    "                df_dewpoint.at[y, x] = dew\n",
    "                df_pressure.at[y, x] = pressure\n",
    "                df_windspeed.at[y, x] = wind\n",
    "                df_ozone.at[y, x] = ozone\n",
    "                df_sunrise.at[y,x] = sunrise\n",
    "                df_sunset.at[y,x] = sunset\n",
    "            \n",
    "            counter = counter + 1\n",
    "            print(counter)\n",
    "\n",
    "            # writing CSVs of what I've got so far, for every write_var API calls\n",
    "            if counter % write_var == 0:\n",
    "                \n",
    "                print('1000 API calls')\n",
    "                df_tMax.to_csv('./csv/tMax_Global.csv', index=False)\n",
    "                df_tMin.to_csv('./csv/tMin_Global.csv', index=False)\n",
    "                df_humidity.to_csv('./csv/humidity_Global.csv', index=False)\n",
    "                df_uvIndex.to_csv('./csv/uv_Global.csv', index=False)\n",
    "                df_cloud.to_csv('./csv/cloud_Global.csv', index=False)\n",
    "                df_precipprob.to_csv('./csv/precip_Global.csv', index=False)\n",
    "                df_dewpoint.to_csv('./csv/dew_Global.csv', index=False)\n",
    "                df_pressure.to_csv('./csv/pressure_Global.csv', index=False)\n",
    "                df_windspeed.to_csv('./csv/wind_Global.csv', index=False)\n",
    "                df_ozone.to_csv('./csv/ozone_Global.csv', index=False)\n",
    "                df_sunrise.to_csv('./csv/sunrise_Global.csv', index=False)\n",
    "                df_sunset.to_csv('./csv/sunset_Global.csv', index=False)\n",
    "\n",
    "# Writing final data to csv\n",
    "print('Final data write')\n",
    "df_tMax.to_csv('./csv/tMax_Global.csv', index=False)\n",
    "df_tMin.to_csv('./csv/tMin_Global.csv', index=False)\n",
    "df_humidity.to_csv('./csv/humidity_Global.csv', index=False)\n",
    "df_uvIndex.to_csv('./csv/uv_Global.csv', index=False)\n",
    "df_cloud.to_csv('./csv/cloud_Global.csv', index=False)\n",
    "df_precipprob.to_csv('./csv/precip_Global.csv', index=False)\n",
    "df_dewpoint.to_csv('./csv/dew_Global.csv', index=False)\n",
    "df_pressure.to_csv('./csv/pressure_Global.csv', index=False)\n",
    "df_windspeed.to_csv('./csv/wind_Global.csv', index=False)\n",
    "df_ozone.to_csv('./csv/ozone_Global.csv', index=False)\n",
    "df_sunrise.to_csv('./csv/sunrise_Global.csv', index=False)\n",
    "df_sunset.to_csv('./csv/sunset_Global.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
